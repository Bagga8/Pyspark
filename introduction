pip install pyspark

from pyspark import SparkConf

from pyspark.context import SparkContext 
conf = SparkConf().setAppName('Pyspark').setMaster('local')
sc = SparkContext(conf=conf)

values = [1,2,3,4,5]
rdd = sc.parallelize(values)

rdd.take(5)

from google.colab import files
uploaded = files.upload()

rdd = sc.textFile("test.txt")

rdd.take(1)

rdd.collect()

aba = sc.parallelize(range(1,10000,2))
aba.persist()

textfile = sc.textFile("test.txt")
textfile.cache()

x = sc.parallelize(["spark", "rdd", "example", "sample", "example"])
y = x.map(lambda x:(x,1))
y.collect()

rdd = sc.parallelize([2,3,4])
sorted(rdd.flatMap(lambda x: range(1,x)).collect())

rdd = sc.parallelize([1,2,3,4,5])
rdd.filter(lambda x: x%2 == 0).collect()

parrallel = sc.parallelize(range(9))
parrallel.sample(True, .2).count()
parrallel.sample(False,1).collect()

parallel = sc.parallelize(range(1,9))
par = sc.parallelize(range(5,15))
parallel.union(par).collect()

parallel = sc.parallelize(range(1,9))
par = sc.parallelize(range(5,15))
parallel.intersection(par).collect()

y = sc.parallelize([5,7,1,3,2,1])
y.sortBy(lambda c: c, True).collect()

z = sc.parallelize([("H", 10), ("A", 26),("F", 3) ,("Z", 1), ("L", 5)])
z.sortBy(lambda c: c, False).collect()

rdd = sc.parallelize([1,2,3,4], 2)
def f(iterator): yield sum(iterator)
rdd.mapPartitions(f).collect()

rdd = sc.parallelize([1,2,3,4], 4)
def f(splitIndex, Iterator): yield splitIndex
rdd.mapPartitionsWithIndex(f).sum()

rdd = sc.parallelize([1,1,2,3,5,8])
result = rdd.groupBy(lambda x: x%2).collect()
sorted ([(x, sorted(y)) for (x,y) in result])

x = sc.parallelize(range(0,3)).keyBy(lambda x: x*x)
y = sc.parallelize(zip(range(0,5), range(0,5)))
[(x, list(map(list, y))) for x, y in sorted(x.cogroup(y).collect())]

x = sc.parallelize(range(0,5))
y = sc.parallelize(range(1000, 1005))
x.zip(y).collect()

sc.parallelize(["a", "b", "c", "d"], 3).zipWithIndex().collect()
